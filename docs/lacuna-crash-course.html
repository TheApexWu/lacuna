<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>LACUNA Crash Course</title>
<style>
  * { margin: 0; padding: 0; box-sizing: border-box; }
  body {
    background: #0a0a0a;
    color: #e5e5e5;
    font-family: 'SF Mono', 'Fira Code', 'Consolas', monospace;
    padding: 40px 20px;
    max-width: 720px;
    margin: 0 auto;
    line-height: 1.7;
  }
  h1 { font-size: 28px; letter-spacing: 6px; color: #f59e0b; margin-bottom: 4px; }
  h1 span { color: #525252; font-size: 14px; letter-spacing: 2px; }
  .subtitle { color: #737373; font-size: 12px; letter-spacing: 2px; margin-bottom: 40px; }
  h2 { font-size: 11px; letter-spacing: 3px; color: #737373; margin-top: 36px; margin-bottom: 12px; text-transform: uppercase; }
  h3 { font-size: 14px; color: #f59e0b; margin-bottom: 6px; }
  p { font-size: 13px; color: #a3a3a3; margin-bottom: 10px; }
  .highlight { color: #e5e5e5; font-weight: bold; }
  .amber { color: #f59e0b; }
  .red { color: #ef4444; }
  .green { color: #22c55e; }
  .blue { color: #3b82f6; }
  .oneliner {
    border-left: 2px solid #f59e0b;
    padding: 12px 16px;
    margin: 20px 0;
    background: rgba(245, 158, 11, 0.05);
    font-size: 14px;
    color: #e5e5e5;
  }
  .step { margin-bottom: 16px; padding-left: 16px; border-left: 1px solid #262626; }
  .step-num { font-size: 10px; color: #525252; letter-spacing: 2px; margin-bottom: 2px; }
  .term {
    display: inline-block; background: rgba(245, 158, 11, 0.1);
    border: 1px solid #262626; border-radius: 3px; padding: 1px 6px; font-size: 12px; color: #f59e0b;
  }
  .viz-table { width: 100%; border-collapse: collapse; margin: 10px 0; font-size: 12px; }
  .viz-table td { padding: 6px 10px; border-bottom: 1px solid #1a1a1a; }
  .viz-table td:first-child { color: #737373; width: 120px; }
  .viz-table td:last-child { color: #a3a3a3; }
  .validation {
    background: rgba(34, 197, 94, 0.05); border: 1px solid #1a3a1a;
    border-radius: 6px; padding: 14px; margin: 14px 0; font-size: 12px;
  }
  .validation p { color: #a3a3a3; margin-bottom: 4px; }
  .vocab { display: grid; grid-template-columns: 110px 1fr; gap: 6px 14px; font-size: 12px; margin: 10px 0; }
  .vocab dt { color: #f59e0b; }
  .vocab dd { color: #a3a3a3; }
  hr { border: none; border-top: 1px solid #1a1a1a; margin: 28px 0; }
  .footer { margin-top: 40px; font-size: 10px; color: #525252; letter-spacing: 1px; }
</style>
</head>
<body>

<h1>LACUNA <span>crash course</span></h1>
<div class="subtitle">Team L'ECART -- Mistral Hackathon NYC 2026</div>

<div class="oneliner">
  "Die Grenzen meiner Sprache bedeuten die Grenzen meiner Welt."<br>
  <span style="color:#737373">-- Wittgenstein, Tractatus 5.6</span><br><br>
  The limits of my language mean the limits of my world.
  When two languages read the same text, they read two different worlds.<br><br>
  <span class="amber">Same text, two languages, different concepts activate. LACUNA measures the gap.</span>
</div>

<h2>The Origin</h2>
<p>
  Words that appear identical across languages do not carry the same meaning.
  Translation gives you equivalent words, not equivalent understanding.
  In context, the same words translated into different languages activate different
  associations, different histories, different weight. No tool existed to show this.
  LACUNA makes the invisible gap visible, measurable, and interactive.
</p>

<h2>The Thesis</h2>
<p>
  Multilingual AI models do not treat concepts equally across languages.
  Some activate stronger in one language than another. That asymmetry reflects
  how each language encodes history, power, and cultural memory.
  <span class="highlight">LACUNA detects, quantifies, and visualizes those asymmetries.</span>
</p>

<h2>Why It Matters</h2>
<p>
  Every multilingual AI system assumes concepts are universal. They are not.
  The Sykes-Picot Agreement processed in English activates mandate, honor, self-determination.
  In Arabic: subjugation, humiliation, resentment. Same text. Same model. Different reading.
  If you deploy AI in 10 languages without checking for this, you are silently collapsing
  distinctions that reshape legal liability, medical risk, and political intent.
</p>

<h2>The Pipeline (5 steps)</h2>
<p>
  1. Extract 43 conceptual frames from the document (Mistral Large).<br>
  2. Define each concept natively in each language -- not translated.<br>
  3. Embed all definitions (BGE-M3, 1024-dim). Project to 2D terrain (UMAP).<br>
  4. User inputs text. Embed it (mistral-embed). Compare against all 43 concept vectors in two languages.<br>
  5. Report <span class="amber">divergence</span> = |activation_lang_a - activation_lang_b|. Sort by gap. The biggest gaps are the findings.
</p>

<hr>

<h2>What's On Screen</h2>

<table class="viz-table">
  <tr><td>Concept</td><td>One of 43 ideas. Each has a native-language definition per language. Shows as labeled text on the terrain.</td></tr>
  <tr><td>Position</td><td>UMAP projection of BGE-M3 vectors (n_neighbors=10, min_dist=0.3, cosine, Procrustes-aligned). Close = semantically similar.</td></tr>
  <tr><td>Height</td><td>Cosine similarity of concept vector to its cluster centroid, scaled [0.2, 1.0], exaggerated nonlinearly. Tall = most representative of its group.</td></tr>
  <tr><td>Color</td><td>Cluster. 6 curated groups: <span class="amber">core</span>, <span class="blue">justice</span>, <span class="green">victory</span>, <span class="red">humiliation</span>, <span style="color:#78716c">lacuna-de</span>, <span style="color:#78716c">lacuna-en</span>. Color only -- zero effect on probes.</td></tr>
  <tr><td>Language switch</td><td>Re-projects from that language's vectors. Terrain reshapes because each language's definitions occupy different regions of embedding space.</td></tr>
  <tr><td>Probe highlight</td><td>Amber glow on top-10 divergent concepts. Dimmed = low divergence. Terrain doesn't change shape -- only labels change.</td></tr>
</table>

<h2>Clusters (what they are, why they exist)</h2>
<p>
  Clusters group related concepts by theme. They are <span class="highlight">curated by the team</span>
  based on the treaty's power dynamics: victors' frame (justice, victory) vs defeated's frame (humiliation).
  On the BGE-M3 model, HDBSCAN (min_cluster_size=3, cosine) computes algorithmic clusters per language.
  Clusters affect <span class="highlight">color only</span>. They do not affect probes, divergence, or any measurement.
</p>
<table class="viz-table">
  <tr><td><span class="amber">Core</span> (4)</td><td>reparations, armistice, honor, treaty</td></tr>
  <tr><td><span class="blue">Justice</span> (10)</td><td>justice, accountability, punishment, debt, guilt, restitution, sanctions, concession, demilitarization, obligation</td></tr>
  <tr><td><span class="green">Victory</span> (9)</td><td>victory, peace, order, triumph, sovereignty, self-determination, legitimacy, diplomacy, reconstruction</td></tr>
  <tr><td><span class="red">Humiliation</span> (11)</td><td>humiliation, betrayal, injustice, revenge, resentment, subjugation, occupation, propaganda, nationalism, starvation, blockade</td></tr>
  <tr><td><span style="color:#78716c">Lacuna-DE</span> (6)</td><td>dolchstoss, schmach, diktat, kriegsschuld, volkszorn, revanchism</td></tr>
  <tr><td><span style="color:#78716c">Lacuna-EN</span> (3)</td><td>magnanimity, civilizing, mandate</td></tr>
</table>

<hr>

<h2>The Probe (the interactive part)</h2>
<p>
  User types or selects text. <span class="highlight">mistral-embed</span> encodes it (1 API call, 1024-dim).
  Cosine similarity computed against all 43 concept vectors in Language A and Language B (86 dot products, &lt;1ms).
  Output: concepts ranked by <span class="term">divergence</span> = |sim_lang_a - sim_lang_b|.
  Direction shows which language pulls harder. Below 0.005 = neutral (noise floor).
</p>

<div class="step">
  <div class="step-num">PRE-LOADED PROBES</div>
  <table class="viz-table">
    <tr><td>Art. 231</td><td>War Guilt Clause, 1919 -- <span class="blue">EN</span> vs <span class="red">DE</span></td></tr>
    <tr><td>Nanking</td><td>Treaty of Nanking, 1842 -- <span class="blue">EN</span> vs <span class="red">ZH</span></td></tr>
    <tr><td>Potsdam</td><td>Potsdam Declaration, 1945 -- <span class="blue">EN</span> vs <span class="red">JA</span></td></tr>
    <tr><td>Sykes-Picot</td><td>Sykes-Picot Agreement, 1916 -- <span class="blue">EN</span> vs <span class="red">AR</span></td></tr>
    <tr><td>38th Parallel</td><td>Korean Partition, 1945 -- <span class="blue">EN</span> vs <span class="red">KO</span></td></tr>
  </table>
</div>

<h2>The Pattern (memorize this)</h2>

<div class="oneliner">
  Same text. Same model. Different conceptual activation. Consistent across 5 texts, 5 language pairs.
</div>

<p>
  <span class="blue">Imperial/colonial language</span> pulls: accountability, mandate, honor, legitimacy, magnanimity, justice<br>
  <span class="red">Colonized/occupied language</span> pulls: humiliation, subjugation, resentment, restitution, debt, revenge
</p>
<p>
  Strongest example: <span class="highlight">Sykes-Picot</span> (EN vs AR).
  English: accountability (+0.067), mandate (+0.059), honor (+0.049).
  Arabic: subjugation (+0.030), humiliation (+0.030), resentment (+0.034).
  Colonial language reads protection. Colonized language reads domination.
</p>

<hr>

<h2>For Mistral Judges (API track framing)</h2>

<div class="validation">
  <p><span class="amber">LACUNA runs on Mistral's stack. Two products in one flow:</span></p>
  <p>1. <span class="highlight">mistral-embed</span> -- every live probe. 1 API call per query. The real-time interactive element.</p>
  <p>2. <span class="highlight">Mistral Large via Agents API</span> -- interpreter agent explains WHY concepts diverge. Extractor agent decomposes documents into conceptual frames.</p>
  <p><span style="color:#737373">BGE-M3 built the terrain map. Mistral is the instrument that takes measurements on the map.</span></p>
</div>

<p><span class="amber">Pitch order for judges:</span></p>
<p>
  1. "We built a real-time interactive product on your API that does something nobody else has done."<br>
  2. "mistral-embed detects cross-lingual divergence in real time." (the live demo)<br>
  3. "Mistral Agents API explains the divergence." (the interpretation)<br>
  4. "We validated across 3 independent models -- Mistral's findings hold across architectures." (the defense)
</p>
<p>
  Lead with product. Demo with the probe. Defend with multi-model validation.
  The multi-model comparison is the answer to the skeptic in Q&A, not the hook.
</p>

<hr>

<h2>What we can and cannot claim</h2>

<div class="validation">
  <p><span class="green">CAN:</span> We built an instrument that surfaces cross-lingual divergence in embedding spaces. Patterns are consistent with known cultural asymmetries across 5 language pairs.</p>
  <p><span class="red">CANNOT:</span> That the model "encodes cultural bias." Some divergence may reflect definition variance (different words), not cultural encoding.</p>
  <p><span class="amber">THE LINE:</span> "Whether this reflects cultural encoding or definition semantics is exactly what this instrument helps you test."</p>
</div>

<h2>Judge Q&A (be ready)</h2>

<div class="step">
  <div class="step-num">Q1: "Why should I care about 0.06?"</div>
  <p>Don't explain math. Fire Sykes-Picot. Let the split land. "The question isn't whether 0.06 is big. It's that the same text activates different concepts at all."</p>
</div>
<div class="step">
  <div class="step-num">Q2: "Isn't this just different definitions?"</div>
  <p>"That's the open research question LACUNA exists to investigate. The patterns are consistent across 5 texts and 5 pairs, aligned with known cultural asymmetries."</p>
</div>
<div class="step">
  <div class="step-num">Q3: "Only one model?"</div>
  <p>"Topology validated across 3 models (BGE-M3, e5, LaBSE), all p &lt; 0.002. Live probe uses a 4th (mistral-embed). Same patterns. Four architectures, one finding."</p>
</div>
<div class="step">
  <div class="step-num">Q4: "Use case?"</div>
  <p>Bias auditing for multilingual AI. Translation QA. Cross-cultural content analysis. Diplomatic comms.</p>
</div>
<div class="step">
  <div class="step-num">Q5: "How is this different from translating?"</div>
  <p>"Translation tells you what words say. LACUNA tells you what words activate. Nanking translates perfectly. But Chinese pulls shame, English pulls magnanimity."</p>
</div>

<hr>

<h2>Demo Script (2 min)</h2>

<div class="step">
  <div class="step-num">0:00 - 0:15 | THE HOOK</div>
  <p>"Wittgenstein said the limits of my language are the limits of my world. When two languages read the same text, they're reading two different worlds. LACUNA shows you exactly where those worlds diverge."</p>
</div>
<div class="step">
  <div class="step-num">0:15 - 0:25 | WHAT IT DOES</div>
  <p>"Give it a sentence and two languages. It tells you what each language hears differently. Not translation. Activation."</p>
</div>
<div class="step">
  <div class="step-num">0:25 - 0:45 | TERRAIN</div>
  <p>Show terrain in English, switch to German. "43 concepts, positioned by embedding proximity, height by centrality. Switch languages -- watch the landscape reshape."</p>
</div>
<div class="step">
  <div class="step-num">0:45 - 1:25 | SYKES-PICOT PROBE (center of demo)</div>
  <p>Click probe. Terrain shifts to Arabic. Let the split land. Pause. Then: "English hears accountability, mandate, honor. Arabic hears subjugation, humiliation, resentment. Same text. Same model. The gap is the finding."</p>
</div>
<div class="step">
  <div class="step-num">1:25 - 1:40 | THE PATTERN</div>
  <p>"5 texts, 5 language pairs. The pattern holds every time. Imperial languages hear justice. Colonized languages hear shame."</p>
</div>
<div class="step">
  <div class="step-num">1:40 - 1:50 | INTERPRETER</div>
  <p>Click a concept. "Mistral's interpreter agent explains why it diverges -- real time, grounded in embedding distances."</p>
</div>
<div class="step">
  <div class="step-num">1:50 - 2:00 | CLOSE</div>
  <p>"Before you deploy AI in 10 languages, LACUNA shows you where your content lands differently. Same text. Different world. We built the instrument that reads the gap."</p>
</div>

<hr>

<h2>Validation</h2>

<div class="validation">
  <p><span class="green">3-model correlation:</span> BGE-M3 vs LaBSE r=0.62, BGE-M3 vs e5 r=0.59, e5 vs LaBSE r=0.47. All p &lt; 0.002. Unanimous lacuna: Dolchstoss.</p>
  <p><span class="green">4 metrics:</span> CLAS (cross-lingual alignment), Mantel test (topology preservation), Silhouette (cluster coherence), Ghost detection rate.</p>
  <p><span class="green">Agentic:</span> Two Mistral Large agents (EN diplomat, DE diplomat) independently reproduce the topology gaps. Embedding geometry and LLM reasoning converge.</p>
</div>

<h2>Vocabulary</h2>

<dl class="vocab">
  <dt>Concept</dt><dd>One of 43 ideas. Native-language definition per language. A data point on the terrain.</dd>
  <dt>Cluster</dt><dd>Color group (6 total). Curated by theme. Color only, no effect on probes.</dd>
  <dt>Weight</dt><dd>cos_sim(concept_vec, cluster_centroid), scaled [0.2, 1.0]. Determines peak height.</dd>
  <dt>Lacuna</dt><dd>Concept present in one language, absent in another. Weight &lt; 0.15 or ratio &gt; 2.5x.</dd>
  <dt>Divergence</dt><dd>|cos(query, lang_a_vec) - cos(query, lang_b_vec)|. The gap.</dd>
  <dt>Direction</dt><dd>Which language a concept leans toward. Blue = lang_a, red = lang_b, grey = neutral.</dd>
  <dt>Probe</dt><dd>Text input. 1 mistral-embed call, 86 cosine comparisons, ranked by divergence.</dd>
  <dt>Topology</dt><dd>UMAP-projected layout. Changes per language because different vectors = different positions.</dd>
  <dt>UMAP</dt><dd>1024-dim to 2D projection. n_neighbors=10, min_dist=0.3, cosine metric, Procrustes-aligned.</dd>
  <dt>mistral-embed</dt><dd>Mistral's embedding model. 1024-dim. Powers live probes.</dd>
  <dt>BGE-M3</dt><dd>BAAI's embedding model. 1024-dim. Powers the terrain.</dd>
  <dt>Interpreter</dt><dd>Mistral Large via Agents API. Explains why a concept diverges.</dd>
</dl>

<h2>Tech Stack</h2>

<dl class="vocab">
  <dt>Mistral Large</dt><dd>Extractor + Interpreter agents via Agents API</dd>
  <dt>mistral-embed</dt><dd>Live probe embeddings (1 call per query)</dd>
  <dt>BGE-M3</dt><dd>Terrain topology (sentence-transformers)</dd>
  <dt>Next.js + Three.js</dt><dd>3D terrain, R3F + postprocessing, real-time transitions</dd>
  <dt>Python</dt><dd>Pipeline: extraction, embedding, UMAP, weight, lacuna detection</dd>
</dl>

<div class="footer">
  L'ECART -- Mistral Worldwide Hackathon -- NYC -- Feb 28-Mar 1, 2026
</div>

</body>
</html>
